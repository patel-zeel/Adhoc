# -*- coding: utf-8 -*-
"""2022-02-09-pytorch-learn-normal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n5-TxAWxTZVlSseU3Uq0LJATvSaWy9RJ

# Learning parameters of a normal distribution in PyTorch

- toc: true 
- badges: true
- comments: true
- author: Nipun Batra
- categories: [ML]
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
sns.reset_defaults()
sns.set_context(context="talk", font_scale=1)
# %matplotlib inline
# %config InlineBackend.figure_format='retina'

dist = torch.distributions

uv_normal = dist.Normal(loc=0.0, scale=1.0)

samples = uv_normal.sample(sample_shape=[1000])

sns.histplot(samples)
sns.despine()

sns.kdeplot(samples, bw_adjust=2)
sns.despine()

uv_normal_dict_mean = {x: dist.Normal(loc=x, scale=1.0) for x in [-2, -1, 0, 1, 2]}

uv_normal_dict_mean_samples = pd.DataFrame(
    {x: uv_normal_dict_mean[x].sample([10000]) for x in uv_normal_dict_mean}
)

sns.displot(uv_normal_dict_mean_samples, kind="kde", fill=True);

samples = uv_normal.sample([10000])
sns.displot(samples, kind="kde")
plt.axvline(0.5, color="k", linestyle="--")
log_pdf_05 = uv_normal.log_prob(torch.Tensor([0.5]))


pdf_05 = torch.exp(log_pdf_05)


plt.title(
    "Density at x = 0.5 is {:.2f}\n Logprob at x = 0.5 is {:.2f}".format(
        pdf_05.numpy()[0], log_pdf_05.numpy()[0]
    )
);

"""#### Learning parameters

Let us generate some normally distributed data and see if we can `learn` the mean.
"""

train_data = uv_normal.sample([10000])

uv_normal.loc, uv_normal.scale

train_data.mean(), train_data.std()

loc = torch.tensor(-10.0, requires_grad=True)
opt = torch.optim.Adam([loc], lr=0.01)
for i in range(3100):
    to_learn = torch.distributions.Normal(loc=loc, scale=1.0)
    loss = -torch.sum(to_learn.log_prob(train_data))
    loss.backward()
    if i % 500 == 0:
        print(f"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc.item():0.2f}")
    opt.step()
    opt.zero_grad()

loc = torch.tensor(-10.0, requires_grad=True)
scale = torch.tensor(2.0, requires_grad=True)

opt = torch.optim.Adam([loc, scale], lr=0.01)
for i in range(3100):
    scale_softplus = torch.functional.F.softplus(scale) # or torch.log(1 + torch.exp(scale))
    to_learn = torch.distributions.Normal(loc=loc, scale=scale_softplus)
    loss = -torch.sum(to_learn.log_prob(train_data))
    loss.backward()
    if i % 500 == 0:
        print(
            f"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc.item():0.2f}, Scale: {scale_softplus.item():0.2f}"
        )
    opt.step()
    opt.zero_grad()

scale_softplus